<!DOCTYPE html>
<html><head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
        <title>Research | Daniel Guest</title>
        <meta name="description" content="Research">
        <meta property="og:site_name" content="Research" />
        <meta property="og:title" content="Daniel Guest" />
        <meta property="og:description" content="PhD student in the Department of Psychology at the University of Minnesota working in Dr. Andrew Oxenham&#39;s Auditory Perception and Cognition Lab and Dr. Kendrick Kay&#39;s Computational Visual Neuroscience Lab. Current research focuses include perception of complex sounds at high frequencies and representation of high-level visual features in the human pulvinar. Interested in perception, neuroscience, statistics, programming, and mathematics."/>
        <meta property="og:image" content="https://guestdaniel.github.io/img/profile.jpg" />
    
    
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    
    <meta name="keywords" content="fast, hugo, theme, minimal, gruvbox">
    <link rel="icon" type="image/svg" href='https://guestdaniel.github.io/img/logo.png'/>
    <meta name="author" content='jane_doe'>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Hugo 0.68.3" />
    <link rel="stylesheet" href='https://guestdaniel.github.io/css/style.css' type="text/css" media="screen" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-2/css/all.min.css" />
    
    </head>

<body>
      <div class="line" id="scrollIndicator"></div>
      <div class="main"><div class="title">
  <div class="name">
    <h2><a href="https://guestdaniel.github.io/"
	   style="text-decoration: none; color: inherit;">Daniel Guest</a></h2>
  </div>
  <div class="color-scheme">
    <input type="checkbox" class="checkbox" id="chk" />
    <label class="label" for="chk">
      <i class="fas fa-moon"></i>
      <i class="fas fa-sun"></i>
      <div class="ball"></div>
    </label>
  </div>
</div>
<script>
  const themeSetter = (theme) => {
      document.body.classList.toggle('light')
      localStorage.setItem('theme', theme)
      blockSwitcher()
  }

  const blockSwitcher = () => [...document.getElementsByTagName("BLOCKQUOTE")]
	.forEach(b => b.classList.toggle('light'))

  const styleSwapper = () => {
      document.body.classList.add('back-transition')
      if (localStorage.getItem('theme') === 'dark') themeSetter('light')
      else if (localStorage.getItem('theme') === 'light') themeSetter('dark')
  }

  if (localStorage.getItem('theme') === 'light'){
      themeSetter('light')
      document.addEventListener("DOMContentLoaded", blockSwitcher)
  }
 else localStorage.setItem('theme', 'dark')

  document.getElementById('chk').addEventListener('change',styleSwapper);

  window.addEventListener("scroll", () => {
      let height = document.documentElement.scrollHeight
          - document.documentElement.clientHeight;
      if(height >= 500){
	  let winScroll = document.body.scrollTop
              || document.documentElement.scrollTop;
	  let scrolled = (winScroll / height) * 100;
	  document.getElementById("scrollIndicator").style.width = scrolled + "%";
      }
  });
</script>

<section class="intro">
    
    <div>
        <h2 id="umn-auditory-perception-and-cognition-lab"><a href="http://apc.psych.umn.edu/">UMN Auditory Perception and Cognition Lab</a></h2>
<p><strong><strong>Time</strong></strong>: September 2017 - present</p>
<p><strong><strong>PI:</strong></strong> Andrew Oxenham</p>
<p>I joined the Auditory Perception and Cognition Lab in September 2017.
My research spans a variety of topics in psychoacoustics and computational modeling of the auditory system.</p>
<h3 id="neural-basis-of-pitch-perception">Neural basis of pitch perception</h3>
<p>My PhD focused on investigating pitch perception with harmonic complex tones composed of low-numbered but high-frequency harmonics (e.g., harmonics 6-10 of a 1400 Hz F0).
Previous research demonstrated that accurate pitch perception is possible with these tones despite the fact that they do not elicit strong phase locking to temporal fine structure in the auditory nerve.
My goal to elucidate the neural code that underlies this phenomenon.
To this end, I am combined psychophysical methods, computational models of the auditory periphery, and ideas from statistical estimation theory (e.g., Cramér–Rao lower bound) to probe the ability of listeners to perceive the pitch of complex tones at high frequencies.</p>
<ul>
<li><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009889">Paper in PLOS Comp Bio (2022)</a></li>
<li><a href="/download/GuestRajappaOxenham2022ARO.pdf">Poster presented at ARO 2022 (virtual)</a></li>
<li><a href="/download/GuestOxenhamARO2021.pdf">Poster presented at ARO 2021 (virtual)</a></li>
<li><a href="/download/GuestOxenhamASA2020.pdf">Poster presented at ASA 2020 (virutal)</a></li>
<li><a href="/download/GuestOxenhamARO2020.pdf">Poster presented at ARO 202</a></li>
<li><a href="/download/GuestOxenhamISAAR2019.pdf">Poster presented at ISAAR 2019</a></li>
<li><a href="/download/GuestOxenhamARO2019.pdf">Poster presented at ARO 2019</a></li>
</ul>
<h3 id="profile-analysis">Profile analysis</h3>
<p>Another topic I have pursued is profile analysis.
In profile-analysis tasks, listeners are asked to identify when one component of a complex sound is incremented in level, even while the sound&rsquo;s overall level is randomized from interval to interval.
I have been exploring how representations of profile-analysis stimuli in the auditory nerve and inferior colliculus, using phenomenological models of the auditory system, relate to psychophysical performance in the task at low and high frequencies.</p>
<ul>
<li><a href="/download/GuestOxenhamARO2022.pdf">Poster presented at ARO 2022 (virtual)</a></li>
</ul>
<h3 id="role-of-pitch-in-the-complex-auditory-scene">Role of pitch in the complex auditory scene</h3>
<p>In my first project in the lab, I examined how F0 differences between a target talker and harmonic complex tone masker benefit speech segregation under a variety of conditions.
This research provided novel insights into how listeners can &ldquo;glimpse&rdquo; target harmonics between resolved masker harmonics.
Most notably, the results pose an interesting challenge for those attempting to build &ldquo;cancellation&rdquo; models of F0-based speech segregation.</p>
<ul>
<li><a href="/download/GuestOxenham2019JASA.pdf">Paper in JASA (2019)</a> <a href="https://doi.org/10.1121/1.5102169">(original link)</a></li>
<li><a href="/download/GuestOxenhamASAVictoria18.pdf">Poster presented at ASA 2018</a></li>
</ul>
<h2 id="umn-computational-visual-neuroscience-lab"><a href="http://cvnlab.net/home.shtml">UMN Computational Visual Neuroscience Lab</a></h2>
<p><strong><strong>Time</strong></strong>: March 2019 - present</p>
<p><strong><strong>PI:</strong></strong> Kendrick Kay</p>
<p>As part of my <a href="http://catss.umn.edu/opportunities.htm">NSF-NRT Graduate Training Program in Sensory Science Fellowship</a>, I am currently pursuing a research project in the Computational Visual Neuroscience Lab.
In this project, I am analyzing data from the <a href="http://naturalscenesdataset.org/">Natural Scenes Dataset</a>, a massive multi-session, multi-subject, high-resolution fMRI dataset collected at the <a href="https://www.cmrr.umn.edu/">Center for Magnetic Resonance Research</a>.
Specifically, I am using encoding models and thalamocortical correlation analyses to explore the functional organization of the human pulvinar.</p>
<h2 id="eriksholm-research-center"><a href="https://www.eriksholm.com/">Eriksholm Research Center</a></h2>
<p><strong><strong>Time</strong></strong>: May 2019 - August 2019</p>
<p><strong><strong>PI:</strong></strong> Lars Bramsløw</p>
<p>During the summer of 2019, I worked as a research intern at Oticon&rsquo;s Eriksholm Research Center.
At Eriskholm, I researched tools and techniques for visualizing and interpreting deep neural networks designed to process and separate speech.</p>
<h2 id="ut-dallas-speech-perception-lab"><a href="https://www.utdallas.edu/~assmann">UT Dallas Speech Perception Lab</a></h2>
<p><strong><strong>Time</strong></strong>: May 2015 - May 2017</p>
<p><strong><strong>PI:</strong></strong> Peter Assmann</p>
<p>As an undergraduate research assistant, my primary role was working on a project investigating perception of indexical properties in children&rsquo;s speech.
Using stimuli from the <a href="https://personal.utdallas.edu/~assmann/KIDVOW1/North_Texas_vowel_database.html">North Texas Voxel Database</a> modified by the STRAIGHT vocoder, I examined how listeners utilize fundamental frequency and formant frequencies when perceiving age and gender in children&rsquo;s speech.
In particular, I focused on conditions of reduced spectrotemporal resolution (through the use of tone vocoders) and on differences between normal-hearing and cochlear-implant listeners.</p>
<ul>
<li><a href="/download/GuestetalASA17.pdf">Poster presented at ASA 2017</a></li>
<li><a href="/download/GuestetalASA2016.pdf">Poster presented at ASA 2016</a></li>
</ul>

    </div>
    
</section>
<footer id="footer">
    <strong></strong>
    <div class="social">
        &nbsp; <a href="mailto:guest121@umn.edu" target="_blank" rel="noopener" title="Email"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg></a> &nbsp;&nbsp; <a href="https://twitter.com/drdrguest" target="_blank" rel="noopener" title="Twitter"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path></svg></a> &nbsp;&nbsp; <a href="https://github.com/guestdaniel" target="_blank" rel="noopener" title="Github"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a> &nbsp;&nbsp; <a href="https://www.linkedin.com/in/guest-daniel/" target="_blank" rel="noopener" title="Linkedin"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle></svg></a> &nbsp;
    </div><strong></strong>
    <p style="color:grey;">© 2021 Daniel Guest.  <a href="https://creativecommons.org/licenses/by/4.0/">Some rights reserved</a>.</p>
</footer>
</div>
    </body>
</html>
